# dynamic_langgraph/nodes/summarizer.py
import asyncio
from typing import AsyncGenerator

from langchain_core.messages import HumanMessage, AIMessage

from ..data_state import DataState
from ..llms import llm_main
from ..utils import split_thought_and_answer
from scripts.recorder import record_and_stream


async def _invoke_llm(prompt: str) -> AIMessage:
    """é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼›å¤šæ•° LLM å®¢æˆ·ç«¯æ˜¯åŒæ­¥çš„ã€‚"""
    # å¦‚æœä½ çš„ llm_main.invoke æœ¬æ¥å°±æ¥å—å­—ç¬¦ä¸²ï¼Œè¿™é‡Œä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼›
    # ä¸ºäº†ä¸å…¶ä»–èŠ‚ç‚¹ä¸€è‡´ï¼Œè¿™é‡Œç”¨ HumanMessage åŒ…è£…ã€‚
    return await asyncio.to_thread(llm_main.invoke, [HumanMessage(content=prompt)])


async def summarizer(state: DataState) -> AsyncGenerator[dict, None]:
    """
    æ±‡æ€»ä¸Šæ¸¸èŠ‚ç‚¹äº§å‡ºï¼Œç”Ÿæˆæœ€ç»ˆæ‘˜è¦ï¼š
      è¾“å…¥: state.user_input, state.features, state.diag, state.viz_summary
      äº‹ä»¶: llm/thought -> result
      è¾“å‡º: {"summary": <str>}
    """
    print("ğŸš© è¿›å…¥ summarizer èŠ‚ç‚¹")

    # å®¹é”™å¤„ç†ï¼Œé¿å… None æ‹¼æ¥æˆ 'None'
    features = getattr(state, "features", None) or {}
    diag = getattr(state, "diag", None) or {}
    viz_summary = getattr(state, "viz_summary", None) or ""

    prompt = (
        "ä½ æ˜¯æœºæ¢°æŒ¯åŠ¨ä¿¡å·åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è¾“å‡ºç»“æ„åŒ–ã€ç®€æ´çš„ç»¼åˆæ€»ç»“ï¼š\n"
        "è¦æ±‚ï¼šå…ˆç»™å‡ºè¦ç‚¹åˆ—è¡¨ï¼Œå†ç»™å‡ºä¸€æ®µå®Œæ•´ç»“è®ºï¼Œé¿å…å †ç Œå†—ä½™ã€‚\n"
        f"- ç”¨æˆ·éœ€æ±‚: {state.user_input}\n"
        f"- ç»Ÿè®¡ç‰¹å¾: {features}\n"
        f"- è¯Šæ–­ç»“æœ: {diag}\n"
        f"- å›¾åƒåˆ†æ: {viz_summary}\n"
    )

    try:
        # 1) è°ƒ LLM
        ai: AIMessage = await _invoke_llm(prompt)
        thought, answer = split_thought_and_answer(ai.content)

        # æ¨é€æ€è€ƒè¿‡ç¨‹ï¼ˆå«æ¨¡å‹/ç”¨é‡ç­‰å…ƒæ•°æ®ï¼‰
        async for event in record_and_stream(
            session_id=state.session_id,
            node="summarizer",
            step_type="llm",
            type="thought",
            thought=thought,
            content=answer,
            model=(getattr(ai, "response_metadata", {}) or {}).get("model"),
            usage=getattr(ai, "usage_metadata", None),
        ):
            yield event

        # 2) æœ€ç»ˆç»“æœäº‹ä»¶
        async for event in record_and_stream(
            session_id=state.session_id,
            node="summarizer",
            step_type="result",
            type="trace",
            result={"summary": answer},
        ):
            yield event

        # 3) ä¼ é€’ç»™ä¸‹æ¸¸
        yield {"summary": answer}

    except Exception as e:
        # å…œåº•å¼‚å¸¸ï¼šæ¨é€ error äº‹ä»¶ï¼Œå¹¶è¿”å›å ä½
        async for event in record_and_stream(
            session_id=state.session_id,
            node="summarizer",
            step_type="exception",
            type="error",
            content=str(e),
        ):
            yield event
        yield {"summary": f"[ERROR] {e}"}
